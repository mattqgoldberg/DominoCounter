{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dominoMap = {\n",
    "    (0,0): 0,\n",
    "    (0,1): 1,\n",
    "    (0,2): 2,\n",
    "    (1,1): 3,\n",
    "    (1,2): 4,\n",
    "    (2,2): 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.GaussianBlur(kernel_size=3),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            #transforms.RandomRotation(30), \n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)), \n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_file_path = self.image_paths[idx]\n",
    "        image = cv2.imread(image_file_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = Image.fromarray(image)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        match = re.search(r'(\\d+)-(\\d+)', image_file_path)\n",
    "        if not match:\n",
    "            raise ValueError(f'Invalid image file path: {image_file_path}')\n",
    "        num1, num2 = map(int, match.groups())\n",
    "\n",
    "        # We are in base 3 here\n",
    "        label = dominoMap[(num1, num2)]\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from multiple directories\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "data_path = kagglehub.dataset_download(\"mattqgoldberg/double-2-dominoes\")\n",
    "print(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from multiple directories\n",
    "\n",
    "dataset_paths = [data_path + '/images']\n",
    "\n",
    "image_paths = []\n",
    "for path in dataset_paths:\n",
    "    for subdir in os.listdir(path):\n",
    "        if os.path.isdir(os.path.join(path, subdir)):\n",
    "            for img in os.listdir(os.path.join(path, subdir)):\n",
    "                image_paths.append(os.path.join(path, subdir, img))\n",
    "            \n",
    "\n",
    "# Split into training and testing sets\n",
    "train_images, test_images = train_test_split(image_paths, test_size=0.2)\n",
    "\n",
    "# Create train and test datasets\n",
    "train_dataset = CustomDataset(train_images, \"\")\n",
    "test_dataset = CustomDataset(test_images, \"\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(len(train_loader.dataset))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "        \n",
    "# Get a random image from the training dataset\n",
    "random_idx = random.randint(0, len(train_dataset) - 1)\n",
    "image_path = train_dataset.image_paths[random_idx]\n",
    "\n",
    "# Load the original image\n",
    "original_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "image = original_image.copy()\n",
    "image = Image.fromarray(image)\n",
    "\n",
    "# Apply transformations\n",
    "transform_pipeline = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.GaussianBlur(kernel_size=3),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            #transforms.RandomRotation(30), \n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.RandomAffine(degrees=0, translate=(0.2, 0.2)), \n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "\n",
    "transformed_image = transform_pipeline(image)\n",
    "\n",
    "# Convert tensors back to images for visualization\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.squeeze(0)  # Remove channel dimension\n",
    "    tensor = tensor * 0.5 + 0.5  # Undo normalization\n",
    "    return tensor.numpy()\n",
    "\n",
    "# Plot original and transformed images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Show original image\n",
    "axes[0].imshow(original_image, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Show transformed image\n",
    "axes[1].imshow(tensor_to_image(transformed_image.T), cmap=\"gray\")\n",
    "axes[1].set_title(\"Transformed Image\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "# Display the images\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipCNN(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(PipCNN, self).__init__()\n",
    "        \n",
    "        # First Convolutional Block\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1_2 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3_2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        # **New Fourth Convolutional Block**\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "\n",
    "        # Pooling Layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # **Global Average Pooling Instead of Flattening**\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        # Fully Connected Layers (Reduced neuron count to prevent overfitting)\n",
    "        self.fc1 = nn.Linear(256, 128)  # Reduced size\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First Block\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn1_2(self.conv1_2(x))))\n",
    "\n",
    "        # Second Block\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn2_2(self.conv2_2(x))))\n",
    "\n",
    "        # Third Block\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn3_2(self.conv3_2(x))))\n",
    "\n",
    "        # Fourth Block (Newly Added)\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Global Average Pooling (GAP)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten along the channel dimension\n",
    "\n",
    "        # Fully Connected Layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "weight_decay = 1e-3\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = PipCNN(num_classes=6).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        correct_predictions += (predictions == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "    \n",
    "    train_accuracy = correct_predictions / total_samples * 100\n",
    "    scheduler.step()\n",
    "\n",
    "    # Run evaluation on the test set\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            test_correct += (predictions == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_accuracy = test_correct / test_total * 100\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# Function to visualize test predictions\n",
    "def visualize_predictions(model, test_loader, class_names, num_images=32):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    images, labels = next(iter(test_loader))  # Get a batch of test images\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)  # Get predictions\n",
    "        _, preds = torch.max(outputs, 1)  # Convert logits to class indices\n",
    "    \n",
    "    images = images.cpu().numpy()\n",
    "    \n",
    "    # Denormalize images for visualization\n",
    "    mean, std = 0.5, 0.5\n",
    "    images = images * std + mean  # Undo normalization\n",
    "\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(30, 40))  # Create a 4x4 grid\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        img = images[idx].squeeze(0)  # Remove extra channel dimension\n",
    "        axes[idx].imshow(img, cmap=\"gray\")\n",
    "        axes[idx].axis(\"off\")\n",
    "        \n",
    "        # Get true and predicted labels\n",
    "        true_label = class_names[labels[idx].item()]\n",
    "        predicted_label = class_names[preds[idx].item()]\n",
    "        \n",
    "        # Set title with color coding\n",
    "        color = \"green\" if true_label == predicted_label else \"red\"\n",
    "        axes[idx].set_title(f\"Pred: {predicted_label}\\nTrue: {true_label}\", color=color, fontsize=40)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Define class names (adjust based on your dataset)\n",
    "class_names = [\"0-0\", \"0-1\", \"0-2\", \"1-1\", \"1-2\", \"2-2\"]  # Update this to match your dataset labels\n",
    "\n",
    "# Call the function to visualize test predictions\n",
    "test_loader_visual = DataLoader(test_dataset, batch_size=16, shuffle=True)\n",
    "visualize_predictions(model, test_loader_visual, class_names, num_images=16)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
